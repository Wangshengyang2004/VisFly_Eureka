VisFly Reward API (draft)
=========================

Purpose
-------
Use this note when writing `get_reward(self) -> torch.Tensor` inside VisFly environments. Rewards must return a 1-D tensor of length `self.num_agent` (one value per drone) on `self.device`. Always stay in torch, keep gradients intact, and avoid in-place ops on tensors returned by the simulator.

Core State Handles
------------------
- `self.num_agent` / `self.num_envs`: total agents simulated in parallel (int).
- `self.device`: torch device where tensors live.
- `self._step_count`: tensor `(num_agent,)` with current step index per agent.
- `self.max_episode_steps`: scalar int limit; use for shaping time-based bonuses.
- `self.position`: tensor `(num_agent, 3)` world positions (meters).
- `self.velocity`: tensor `(num_agent, 3)` linear velocity (m/s). Use `(self.velocity - 0)` to keep gradients.
- `self.acceleration`: tensor `(num_agent, 3)` world-frame acceleration.
- `self.angular_velocity`: tensor `(num_agent, 3)` body angular rates (rad/s).
- `self.angular_acceleration`: tensor `(num_agent, 3)` angular accelerations.
- `self.orientation`: tensor `(num_agent, 4)` when dynamics output quaternions; order `[w, x, y, z]`. Some envs expose orientation helpers (see below).
- `self.direction`: tensor `(num_agent, 3)` forward (body x-axis) unit vector in world frame.
- `self.thrusts`: tensor `(num_agent, 4)` motor thrust commands (Newtons).
- `self.state`: concatenation of `[position, orientation, velocity, angular_velocity]`.
- `self.full_state`: extended state `[position, orientation, velocity, angular_velocity, motor_omega, thrusts, t]`.
- `self.t`: tensor `(num_agent,)` simulation time (seconds).

Collision + Scene Awareness
---------------------------
- `self.is_collision`: bool tensor `(num_agent,)` flagged when the drone intersects geometry or exits bounds.
- `self.collision_vector`: tensor `(num_agent, 3)` pointing from the drone toward the active collision point. It is zero when no obstacle is nearby, and its norm equals `self.collision_dis`.
- `self.collision_dis`: tensor `(num_agent,)` Euclidean distance to the nearest obstacle (meters). Values shrink toward `0` at impact and increase above `1` when safely clear.
- `self.collision_point`: tensor `(num_agent, 3)` world coordinates of the closest surface point retained from the last collision update.
- `self.dynamic_object_position` / `self.dynamic_object_velocity`: lists of tensors for moving obstacles when scene spawns them.
- There is no occupancy grid or action history tensor—rely on the above vectors for safety shaping.

Sensors
-------
`self.sensor_obs` maps sensor `uuid` to raw outputs (torch or numpy arrays). Common keys:
- `"depth"`: shape `(num_agent, H, W)` or `(num_agent, 1, H, W)` in meters.
- `"color"`: shape `(num_agent, 3, H, W)` RGB uint8.
- Additional sensors (e.g., semantic) follow Habitat naming; check `sensor_kwargs` per env.
Keep depth/color tensors on `self.device` with `th.as_tensor` before use.

Orientation Utilities
---------------------
`self.orientation` is a quaternion helper from `VisFly.utils.maths.Quaternion`.
Key methods (access through the object returned by `self.envs.dynamics._orientation` or directly in some envs):
- `orientation.world_to_head(vec.T).T`: rotate world-frame vectors into yaw-aligned heading frame.
- `orientation.toEuler()` returns roll/pitch/yaw.
- `self.direction` already provides forward axis; use for alignment rewards.

Common Reward Patterns
----------------------
- Centering: `(self.position - target).norm(dim=1)` gives Euclidean distance.
- Velocity control: `(self.velocity - 0).norm(dim=1)` penalizes speed while keeping gradients.
- Smooth orientation: `(self.angular_velocity - 0).norm(dim=1)`.
- Collision margin: use `self.collision_dis` and `self.collision_vector` for avoidance. Example safe penalty: `-k * th.exp(-alpha * (self.collision_dis - margin))`.
- Progress: `(self.velocity * desired_direction).sum(dim=1)` encourages motion toward goals.

Environment-Specific Fields
---------------------------
HoverEnv / HoverEnv2:
- `self.target`: `(num_agent, 3)` hovering point (meters).
- `self.success_radius`: float tolerance for success detection.
- HoverEnv2 exposes depth via `sensor_obs["depth"]` scaled by `/10` inside observations.

NavigationEnv:
- `self.target`: `(num_agent, 3)` waypoint (default `[15, 0, 1.5]`).
- `self.direction`: reused for heading alignment; `self.collision_vector` is active in tight spaces.
- When depth sensor is enabled, use `self.sensor_obs["depth"]` if you need obstacle cues (prefer `self.collision_vector` for cheap safety).

RacingEnv:
- `self.targets`: tensor `(num_gates, 3)` gate centers.
- `_next_target_i`: `(num_agent,)` index of the upcoming gate.
- `_is_pass_next`: bool mask when a gate has been cleared.
- Reward ideas: distance/velocity toward `self.targets[self._next_target_i]`, bonus on `self.is_pass_next`.

TrackEnv / TrackEnv2:
- `self.target`: `(num_agent, next_points_num, 3)` future waypoints on a circular track.
- `self.center`, `self.radius`, `self.radius_spd`: track geometry constants.
- Use `self.target[:, 0, :]` for immediate waypoint; flatten future differences for smoothing.

LandingEnv / LandingEnv2:
- `self.target`: landing pad center (z=0 for env1, elevated for env2).
- `self.centers`: `(num_agent, 2)` pixel offsets from pad center extracted from color mask; becomes `None` until first observation.
- `self.success_radius`: horizontal tolerance.
- Access `self.sensor_obs["color"]` (uint8) if you need to recompute pad alignment.

MultiNavigationEnv:
- `self.target`: `(num_agent, 3)` per-drone goal.
- `self.path`: list of path waypoints when `scene_kwargs['is_find_path']` is enabled.
- `swarm` observations assemble teammate states; reward functions can derive spacing from `self.state` via agent indexing.

General Safety Notes
--------------------
- Only touch attributes listed in this document or surfaced directly by the provided environment stub. Anything else (e.g., `self.action`, `self.actions`, `self.action_diff`) does not exist.
- Reward functions must not wrap access in `hasattr` / `if attr exists` branches—prefer failing fast over silently masking missing data.
- Always operate on clones or expressions like `(tensor - 0)` to avoid modifying simulator buffers.
- Avoid python floats inside `torch.where`; pass scalar floats, not new tensors with device mismatches.
- Clamp divisions with small epsilon (`1e-6` or `1e-8`) to keep gradients finite.
- Never change `self.*` buffers in-place (e.g., `self.position +=` is forbidden).
- Keep outputs finite: guard against `nan`/`inf` using `torch.where` or clamping.
- Respect device: wrap constants using `torch.full((self.num_agent,), value, device=self.device)` when you need per-agent scalars.

Useful Scalars
--------------
- `self.envs.dynamics.ctrl_dt`: control timestep (seconds) accessible via `self.envs.dynamics.ctrl_dt` if shaping per-step energy.
- `self.envs.dynamics.action_type`: string (`"bodyrate"`, `"velocity"`, etc.) if reward depends on control mode.
- `self.is_eval`: some envs expose evaluation flag for deterministic shaping (e.g., LandingEnv).

Coding Checklist
----------------
- Return tensor shape `(num_agent,)`, dtype float32, device = `self.device`.
- Compose from torch ops only; no numpy/scipy inside rewards.
- Reuse helper vectors (`self.direction`, `self.collision_vector`) rather than re-deriving heavy geometry.
- Logically separate reward terms for distance, smoothness, safety, and bonuses so the evaluator can surface components if needed.
