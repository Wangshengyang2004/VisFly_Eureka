"""
Simple Usage Example for VisFly-Eureka Integration

This example demonstrates how to use the basic functionality of the
VisFly-Eureka integration for reward function optimization.
"""

import sys
import torch
import logging

# Add project paths
sys.path.insert(0, '/home/simonwsy/VisFly_Eureka')
sys.path.insert(0, '/home/simonwsy/VisFly_Eureka/VisFly')

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')


def simple_example_with_mock_env():
    """Demonstrate basic usage with a mock environment."""
    print("=" * 60)
    print("VisFly-Eureka Integration - Simple Example")
    print("=" * 60)
    
    # Import components
    from eureka_visfly import EurekaVisFly, OptimizationConfig
    
    # Define a simple mock environment for demonstration
    class SimpleNavigationEnv:
        """Mock navigation environment for demonstration."""
        
        def __init__(self, num_agent_per_scene=1, device="cpu", target=None, **kwargs):
            self.num_agent = num_agent_per_scene
            self.device = device
            self.target = target if target is not None else torch.tensor([[5.0, 0.0, 1.5]])
            self.position = torch.zeros(self.num_agent, 3)
            self.velocity = torch.zeros(self.num_agent, 3)
            self.max_episode_steps = kwargs.get("max_episode_steps", 100)
            
            # Simulate depth sensor data
            self.sensor_obs = {
                "depth": torch.ones(self.num_agent, 32, 32) * 2.0  # 2 meters depth
            }
            
        def reset(self):
            """Reset environment to initial state."""
            self.position = torch.randn(self.num_agent, 3) * 0.1  # Start near origin
            self.velocity = torch.zeros(self.num_agent, 3)
            return {"state": torch.cat([self.position, self.velocity], dim=1)}
            
        def step(self, action):
            """Simulate one environment step."""
            # Simple physics simulation
            self.velocity += action * 0.1
            self.position += self.velocity * 0.02
            
            # Get observation
            obs = {"state": torch.cat([self.position, self.velocity], dim=1)}
            
            # Get reward from current reward function
            reward = self.get_reward()
            
            # Simple termination condition
            done = torch.norm(self.position - self.target, dim=1) < 0.1
            
            return obs, reward, done.any(), {}
            
        def get_reward(self):
            """Default reward function (will be replaced by generated ones)."""
            # Simple distance-based reward
            distance = torch.norm(self.position - self.target, dim=1)
            return -distance * 0.1

    print("1. Setting up mock environment and configuration...")
    
    # Configure the optimization
    config = OptimizationConfig(
        iterations=2,        # Run 2 optimization iterations
        samples=3,           # Generate 3 reward functions per iteration
        training_steps=50,   # Minimal training for demonstration
        algorithm="bptt",    # Use BPTT algorithm
        evaluation_episodes=5  # Evaluate with 5 episodes
    )
    
    # Environment configuration
    env_kwargs = {
        "num_agent_per_scene": 1,
        "device": "cpu",
        "target": torch.tensor([[3.0, 1.0, 1.0]]),  # Navigation target
        "max_episode_steps": 50
    }
    
    print("2. Creating EurekaVisFly controller...")
    
    # Create the Eureka controller (using mock LLM for demonstration)
    eureka = EurekaVisFly(
        env_class=SimpleNavigationEnv,
        task_description="Navigate drone to target position at [3, 1, 1] while avoiding obstacles using sensors",
        llm_config={"model": "mock-gpt-4"},  # Mock LLM for demonstration
        env_kwargs=env_kwargs,
        optimization_config=config,
        device="cpu"
    )
    
    print(f"âœ“ Controller created with {type(eureka.llm).__name__}")
    
    print("\n3. Testing environment creation...")
    
    # Test environment creation
    test_env = eureka.create_environment()
    print(f"âœ“ Environment created: {test_env.__class__.__name__}")
    
    # Test environment functionality
    obs = test_env.reset()
    print(f"âœ“ Environment reset successful, obs shape: {obs['state'].shape}")
    
    original_reward = test_env.get_reward()
    print(f"âœ“ Original reward: {original_reward}")
    
    print("\n4. Testing reward function generation...")
    
    # Generate reward function candidates
    reward_functions = eureka.generate_reward_candidates(samples=2, iteration=0)
    print(f"âœ“ Generated {len(reward_functions)} reward function candidates")
    
    if reward_functions:
        print("\nFirst generated reward function:")
        print("-" * 40)
        print(reward_functions[0])
        print("-" * 40)
        
        print("\n5. Testing reward injection...")
        
        # Test reward injection
        from eureka_visfly.reward_injection import inject_generated_reward
        
        test_env2 = eureka.create_environment()
        success = inject_generated_reward(test_env2, reward_functions[0])
        
        if success:
            test_env2.reset()
            new_reward = test_env2.get_reward()
            print(f"âœ“ Reward injection successful! New reward: {new_reward}")
            
            # Compare with original
            if not torch.allclose(new_reward, original_reward):
                print("âœ“ Reward function successfully replaced")
            else:
                print("! Reward function appears unchanged")
        else:
            print("âœ— Reward injection failed")
            
    print("\n6. Demonstrating optimization workflow...")
    
    # Note: We don't run the full optimization here as it requires more complex setup
    # But we can show how the components work together
    
    print("ðŸ“ Optimization workflow overview:")
    print("   1. Generate reward functions using LLM")
    print("   2. Inject reward functions into environment instances")
    print("   3. Train policies using BPTT or PPO")
    print("   4. Evaluate trained policies")
    print("   5. Rank reward functions by performance")
    print("   6. Iterate with feedback to improve rewards")
    
    print("\n" + "=" * 60)
    print("ðŸŽ‰ Phase 1 Implementation Complete!")
    print("=" * 60)
    print()
    print("âœ“ Core Components Implemented:")
    print("  - EurekaVisFly: Main orchestrator class")
    print("  - LLMEngine & MockLLMEngine: Reward function generation")
    print("  - RewardInjector: Direct reward injection into environments")
    print("  - TrainingUtils: Training and evaluation support")
    print("  - Prompts: LLM prompt engineering for VisFly")
    print()
    print("âœ“ Key Features Working:")
    print("  - Direct reward injection without wrapper layers")
    print("  - LLM-powered reward generation with context awareness")  
    print("  - Integration with VisFly's native training systems")
    print("  - Comprehensive error handling and fallback mechanisms")
    print("  - Mock components for development and testing")
    print()
    print("ðŸš€ Ready for Phase 2: Training & Evaluation!")


def show_api_usage():
    """Show the main API usage patterns."""
    print("\n" + "=" * 60)
    print("API Usage Examples")
    print("=" * 60)
    
    print("\n1. Basic Usage:")
    print("""
from eureka_visfly import EurekaVisFly
from VisFly.envs.NavigationEnv import NavigationEnv

# Create Eureka controller
eureka = EurekaVisFly(
    env_class=NavigationEnv,
    task_description="Navigate to target avoiding obstacles", 
    llm_config={"model": "gpt-4", "api_key": "your-key"}
)

# Run optimization
best_rewards = eureka.optimize_rewards(iterations=5, samples=16)
print(f"Best reward score: {best_rewards[0].score():.3f}")
""")
    
    print("\n2. Direct Reward Injection:")
    print("""
from eureka_visfly.reward_injection import inject_generated_reward

# Create environment
env = NavigationEnv(num_agent_per_scene=4, device="cuda")

# Inject custom reward
reward_code = '''
def get_reward(self):
    distance = torch.norm(self.position - self.target, dim=1)
    return -distance * 0.1 + collision_avoidance_bonus
'''

inject_generated_reward(env, reward_code)
""")
    
    print("\n3. Mock Development:")
    print("""
from eureka_visfly import EurekaVisFly

# Use mock LLM for development (no API key needed)
eureka = EurekaVisFly(
    env_class=YourEnv,
    task_description="Your task",
    llm_config={"model": "mock-gpt-4"}  # Automatically uses MockLLMEngine
)
""")


if __name__ == "__main__":
    try:
        simple_example_with_mock_env()
        show_api_usage()
    except Exception as e:
        print(f"\nâŒ Example failed: {e}")
        import traceback
        traceback.print_exc()