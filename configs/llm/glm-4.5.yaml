llm:
  vendor: "bigmodel"
  model: "glm-4.5"

  # Generation parameters
  temperature: 1.0
  max_tokens: 8192
  top_p: 0.95
  frequency_penalty: 0.0
  presence_penalty: 0.0

  # Request settings
  timeout: 520
  max_retries: 3
  request_delay: 1.0      # Conservative for stability
  batch_size: 5           # Conservative batch size

  # Advanced settings
  use_chat_completion: true
  system_message_enabled: true
  conversation_memory: true
  
  # GLM-specific settings
  thinking:
    enabled: false          # Disable thinking chain for faster responses
  
  # Batching strategy
  batching:
    supports_n_parameter: false    # GLM doesn't support n parameter
    strategy: "async"         # Options: "n_parameter", "sequential", "async", "multiprocessing"
    max_concurrent: 10              # For async/multiprocessing strategies