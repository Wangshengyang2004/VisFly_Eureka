llm:
  vendor: "bigmodel_coding"
  model: "glm-4.7"
  temperature: 1.0
  timeout: 120
  max_retries: 3
  thinking:
    enabled: true
  batching:
    strategy: "sequential"

