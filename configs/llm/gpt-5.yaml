llm:
  vendor: "ifopen"
  model: "gpt-5"

  # Generation parameters
  temperature: 1.0
  max_tokens: 8192
  top_p: 0.95
  frequency_penalty: 0.0
  presence_penalty: 0.0

  # Request settings
  timeout: 520
  max_retries: 3
  request_delay: 1.0      # Conservative for stability
  batch_size: 5           # Conservative batch size

  # Advanced settings
  use_chat_completion: true
  system_message_enabled: true
  conversation_memory: true

  thinking:
    enabled: false          # Disable thinking chain for faster responses
  
  # Batching strategy
  batching:
    supports_n_parameter: true
    strategy: "n_parameter"         # Options: "n_parameter", "sequential", "async", "multiprocessing"
    max_concurrent: 5              # For async/multiprocessing strategies